{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the machine learning stack.\n",
    "\n",
    " I'm a starter in machine learning. In a few days' I'll be a bad ass, telling a different story.\n",
    "\n",
    "### Machine learning workflow\n",
    "\n",
    "Ask the right question.\n",
    "Prepare the data needed to ask the question.\n",
    "Selecting the algorithmn you want to use.\n",
    " - The question asked will help us in selecting the algorithmn that we would like to use.\n",
    "Identify the training model.\n",
    " - From the data provided select a subset of the data obtained for training. This will help in predicting results on similar data.\n",
    "Testing the model.\n",
    " - This is used to test the data that was not used for training.\n",
    " \n",
    "Note: The earlier steps are the most important. Making a mistake on this steps will make you go back and re-define them to get good results.\n",
    "Expect to use a considerable amount of time in redifining the models selected in order to get good results.\n",
    "\n",
    "### Solution statement goals.\n",
    "Define the goal(including data goals).\n",
    "Define target performance.\n",
    "Define context for usage.\n",
    "Define how solution will be created.\n",
    "\n",
    "### Our problem statement\n",
    "\"Predict if a person will develop diabetes.\"\n",
    "  - Scope and data sources.\n",
    "  Getting the data from America's diabetes website.(Originaly from UCI machine learning repository.)\n",
    "   - Analyse the data and get the key features that will make sense in our analysis.(i.e identify critical features)Focus on at a risk population.\n",
    "   - Select an appropriate data source.(Pima indian diabetes study is a good source.)In our case the problem statement will be:\n",
    "   \"Using pima indian diabetes data set predict which people will develop diabetes.\"\n",
    "   - Using a binary result prediction is likely to get  a 50% accuracy which is a bit low.\n",
    "   - Genetic difference are a big factor.This might give us a 70% accuracy. This might change the problem statement to: \"Using pima indian diabetes data, predict with 70% or great accuracy, which people will develop diabetes.\"\n",
    "   \n",
    " #### Context.\n",
    " We're doing a disease prediction. Are we sure the prediction will be accurate?\n",
    " Medical research context is also very critical.\n",
    " Unknown variations between people.\n",
    " Likelihood is used. The problem statement will change to: \": \"Using Pima Indian diabetes data, predict with 70% or great accuracy, which people are likely to develop diabetes.\"\n",
    " \n",
    "### Solution creation\n",
    "Machine learning workflow.\n",
    " - Process Pima Indian data.\n",
    " - Transform data as required.\n",
    "The problem statement will change to : \"Use the Machine learning workflow to process and transform Pima Indian data to create a prediction model. This model must predict which people are most likely to develop diabetes with 70% or greater accuracy. \"\n",
    " Note: The problem statement now has the data we will use, the perfomance expected and how we will go about creating a solution.\n",
    "\n",
    "### Once the data is found\n",
    "Inspect and clean the data.\n",
    "Explore the data.\n",
    "Mold the data to tidy data.\n",
    "\n",
    "Def: Tidy data- Tidy datasets are easy to manipulate, model and visualize, and have a specific structure:\n",
    " - Each variable is a column.\n",
    " - Each observation is a row.\n",
    " - Each type of an observational unit is a table.\n",
    " \n",
    " Xtics:\n",
    " - It's more pragmatic than other forms of data.\n",
    " - Focuses on presenting data in a clean readable format.\n",
    " 50 - 80% of machine learning is spent in getting, cleaning and organizing data.\n",
    " \n",
    " ##### Data sources\n",
    " Data sources can differ significantly. Some of the data sources are:\n",
    " * Google.\n",
    " * Government data sources.\n",
    " * Professional of company data sources.\n",
    " * Your company. (Data can be obtained from the IT staff.)\n",
    " * Your department. (Department can maintain department specific data.)\n",
    "Jupyter notebook - Launches the notebook server. \n",
    " Xtics of the PIMA indian diabetes data:\n",
    " * Female patients atleast 21 years old.\n",
    " * 768 patient observation rows.\n",
    " * 10 columns.\n",
    "   - 9 feature columns : Number of pregnancies, blood pressure, glucose, insulin level, ...\n",
    "   - 1 class column: Diabetes (True or False)\n",
    " Data rules\n",
    "  #1 The closer the data is to what you are predicting the better.\n",
    "  #2 Data will never be in the format you need.\n",
    " Columns to eliminate:\n",
    "     Not used\n",
    "     No values\n",
    "     Duplicates\n",
    " Eliminating co-related columns.\n",
    "    They contain the same information in different format.i.e Id and value associated withe the id.\n",
    "    Add little information.\n",
    "    Causes algorithmn to get confused.\n",
    "  \n",
    " Some small steps:\n",
    "  - Load the data.\n",
    "  - Explore the data - To show the kind of information it provides.\n",
    "  - Clean the data.\n",
    "  \n",
    "  Pandas dataframe\n",
    "  It has dataframes that contain column names and rows.\n",
    "  > shape - returns the number of rows and columns inform of a tuple.\n",
    "  > head(5) - returns the first five rows of the data.\n",
    "  > tail(5) - returns the last five rows of the data.\n",
    "  > isnull().values.any() - It checks if there is any null value in the dataframe.The any() returns true if a dataframe is empty.\n",
    "  NB: The rows are index based - The first row is index 0 the last row is the total_rows -1\n",
    " The columns represent the logical features of the data.\n",
    "\n",
    "Use the head() function to check the first rows of the the data.\n",
    "- If we pass a parameter to the head function it will give a subset of the first rows of the data.\n",
    "  > e.g. head(5) - this returns the first five rows of the data. Note that the rows are index based.\n",
    "The tails works similarly to the head function although they return the last rows of the data.\n",
    " > e.g. tail(5) returns the last five rows.\n",
    " \n",
    "Pandas data frames have columns so we can use the column names to access data. The columns names represent logical \n",
    "features of the data at stake.\n",
    "When analysing the data there is need to eliminate certain data in our mass. Some of the characteristic data to eliminate are:\n",
    " > Columns that are not being used. i.e Some features are irrelevant for us to achieve what we want.\n",
    " > Columns that have no values.\n",
    " > Columns that contain duplicate data.\n",
    " > Correlated columns. These are columns that contain the same information in a different format.\n",
    "   - Id and values associated with Id.\n",
    "   \n",
    "df.corr() - this function returns the correlated columns. In our example 1.0 shows rows that strongly co-related. In this case there is need to eliminate one of the columns that brings the issue.\n",
    "del df['column']- It deletes the column that we is bringing up the redundancy in our data.\n",
    "After the correlated columns are eliminated we can now proceed to moulding our data.\n",
    "Moulding the data involves creating new columns, creating new columns if need be. It involves making all the data types uniform in all the columns.\n",
    "df.map()- this function is used to map our data for it to follow a given convention. For instance pass a dictionary with the data that you want mapped. This dictionary can be passed to our map function and it does us the replacement for us. For instance {False: 1}. This replace all the false in the data with 1.\n",
    "\n",
    "#### Selecting an algorithmn.\n",
    "- Role of the algorithmn\n",
    "- Perform algorithmn selection.\n",
    "Use solution statement to filter algorithmns.\n",
    "Discuss best algorithmns.\n",
    "Select one initial algorithmn.\n",
    "An algorithmn is an engine that drives the entire process.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Diabetes\n",
    "### Import Libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ./data/pima-data.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4811fbf836d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# do ploting inline instead of in a separate window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/pima-data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angiemutava/.virtualenvs/mlearning/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angiemutava/.virtualenvs/mlearning/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angiemutava/.virtualenvs/mlearning/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angiemutava/.virtualenvs/mlearning/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angiemutava/.virtualenvs/mlearning/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ./data/pima-data.csv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd # its a dataframe library\n",
    "import matplotlib.pyplot as plt # matplotlib plots data\n",
    "import numpy as np # it provides the n-dimension\n",
    "\n",
    "# do ploting inline instead of in a separate window\n",
    "% matplotlib inline\n",
    "df = pd.read_csv(\"./data/pima-data.csv\")\n",
    "df.shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72b44c061663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
